{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0XcItpFbS6Qi"
   },
   "source": [
    "<b><h1 style=\"font-size:28px;\"> Emotion Recognition :</h1></b>\n",
    "\n",
    "### By - Ashavidya Kusuma\n",
    "#### introduction\n",
    "\n",
    ">Predict human emotion using webcam and pytorch. This will involve a passing the test image as the video in real time and determine the emotions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context\n",
    "> Emotions detection has been something  which seemed unachievable for some time, but with <code>provision of computing power </code>, this confirms that emotion detection can now be  implemented in many areas requiring additional security or information about the person.\n",
    "\n",
    "### Rationale\n",
    "\n",
    ">This notebook confirms that through pytorch we can actually get the emotions  through just taking the picture of a person. We will identify the emotion of a person by taking the input inform of video from the webcam\n",
    "\n",
    "### Aims and objectives\n",
    "\n",
    ">The aim of this project is to determine whether a person is Angry, Disgusted, scared, Happy,Sad,Surprise or Neutral based on the input image from the webcam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bd9nWNnLTeDX"
   },
   "source": [
    "### Importing libraries:\n",
    "\n",
    "#### numpy\n",
    ">NumPy is a python library used for working with arrays.\n",
    "It also has functions for working in domain of linear algebra, fourier transform, and matrices\n",
    "#### pandas\n",
    "> will be used in  data munging/wrangling \n",
    "\n",
    "### searbon\n",
    "> Will be used in data visualization\n",
    "\n",
    "### torch\n",
    ">PyTorch is a Python package that provides two high-level features:\n",
    "\n",
    ">1. Tensor computation (like NumPy) with strong GPU acceleration\n",
    "\n",
    ">2. Deep neural networks built on a tape-based autograd system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "colab_type": "code",
    "id": "gCc6p_8KS6Qj",
    "outputId": "90c0cbdf-437a-456a-829e-573a73370ee5"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models.resnet import ResNet, BasicBlock\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torchvision import models\n",
    "from torch.autograd import Variable\n",
    "from random import randint\n",
    "from tqdm.autonotebook import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "plt.ion()   # interactive mode\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AnWpE_z7S6Qo"
   },
   "source": [
    "### Loading the data :\n",
    "\n",
    "The data used, fer2013.csv was extracted from one the face detection challenges from kaggle.\n",
    "#### Description of the dataset.\n",
    "<ul>\n",
    "<li><h4>Dataset properties</h4></li>\n",
    "<li>This dataset consists of 48x48 pixel grayscale images </li>\n",
    "    <li>Breakdown of the dataset properties</li>\n",
    "<li>0: -4593 images- Angry</li>\n",
    "<li>1: -547 images- Disgust</li>\n",
    "<li>2: -5121 images- Fear</li>\n",
    "<li>3: -8989 images- Happy</li>\n",
    "<li>4: -6077 images- Sad</li>\n",
    "<li>5: -4002 images- Surprise</li>\n",
    "<li>6: -6198 images- Neutral</li>\n",
    "    \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "FhthoUoqS6Qp",
    "outputId": "71cf59af-15ce-4471-9670-4f7fcc369db1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>pixels</th>\n",
       "      <th>Usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   emotion                                             pixels     Usage\n",
       "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
       "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
       "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
       "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
       "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('fer2013.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BjquzHTXS6Qu"
   },
   "source": [
    "### Random Model :\n",
    ">Test the input data if its able to make predictions through a random model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8m-ss0YpS6Qv"
   },
   "outputs": [],
   "source": [
    "em = np.array(data['emotion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "EADfVgT0S6Qz",
    "outputId": "207bde12-0758-43b6-9f2e-9852f388219c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14284141889820828\n"
     ]
    }
   ],
   "source": [
    "avg_error = 0\n",
    "for i in range(1000):\n",
    "    rpred = [randint(0,6) for i in range(len(em))]\n",
    "    x = em == rpred\n",
    "    x = sum(x)\n",
    "    avg_error += x*1.0/len(em)\n",
    "\n",
    "print(avg_error/1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HptBntilS6Q4"
   },
   "source": [
    "### Preprocessing the data :\n",
    "\n",
    "#### Steps\n",
    "> The first step is to change type to numpy array, the reshapeImagepixels to make the model training more easier\n",
    "\n",
    "> The changeTypeToNumpyArray() converts the input image to its RGB form\n",
    "\n",
    "> The reshapeImagePixels() is used to reshape the image for easier training and more accuracy during training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BZc8HEUiS6Q8"
   },
   "outputs": [],
   "source": [
    "# Convert string of pixels to list\n",
    "def changeTypeToNumpyArray(x):\n",
    "    x = x.split()\n",
    "    x = [int(i) for i in x]\n",
    "    return x\n",
    "\n",
    "# Reshape the image to (1, 48,48)\n",
    "def reshapeImagePixels(x):\n",
    "    x = np.array(x)\n",
    "    x = x/255\n",
    "    x = x.reshape(1, 48, 48)\n",
    "    return x\n",
    "\n",
    "def preprocess(data):\n",
    "    data['pixels'] = data['pixels'].apply(changeTypeToNumpyArray)\n",
    "    data['pixels'] = data['pixels'].apply(reshapeImagePixels)\n",
    "    processed_data = data[['pixels', 'emotion']]\n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6SI4FwafS6RB"
   },
   "source": [
    "### pass the training data into the proccessing function defined above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "59MqRDN8S6RF",
    "outputId": "8c63ba12-3d5c-4ee2-b580-e5269185e6bd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixels</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[[0.27450980392156865, 0.3137254901960784, 0....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[[0.592156862745098, 0.5882352941176471, 0.57...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[[0.9058823529411765, 0.8313725490196079, 0.6...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[[0.09411764705882353, 0.12549019607843137, 0...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[[0.01568627450980392, 0.0, 0.0, 0.0, 0.0, 0....</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              pixels  emotion\n",
       "0  [[[0.27450980392156865, 0.3137254901960784, 0....        0\n",
       "1  [[[0.592156862745098, 0.5882352941176471, 0.57...        0\n",
       "2  [[[0.9058823529411765, 0.8313725490196079, 0.6...        2\n",
       "3  [[[0.09411764705882353, 0.12549019607843137, 0...        4\n",
       "4  [[[0.01568627450980392, 0.0, 0.0, 0.0, 0.0, 0....        6"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get Training Data\n",
    "train_data = preprocess(data)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This step nvolves converting the dataset format to enable us use it with pytorch\n",
    "#### Steps\n",
    "> Convert data to numpy array\n",
    "\n",
    "> Convert to tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EBZ953wbS6RN"
   },
   "outputs": [],
   "source": [
    "# Converting to dataset format for pytorch use :\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        # Convert data to numpy array.\n",
    "        self.images = np.array(data['pixels'])\n",
    "        self.labels = np.array(data['emotion'])\n",
    "        # Convert to Tensors\n",
    "        print(self.images.shape)\n",
    "        print(self.labels.shape)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # Get item at index location\n",
    "        img = self.images[index]\n",
    "        # Convert to tensor.\n",
    "        img = torch.from_numpy(img)\n",
    "        label = torch.from_numpy(self.labels)\n",
    "        label = label[index]\n",
    "        # return tesnor.    \n",
    "        return (img, label)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.images.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "4m1TVObuS6RR",
    "outputId": "399c5e05-9f64-474a-ce17-60da1c92fa34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35887,)\n",
      "(35887,)\n"
     ]
    }
   ],
   "source": [
    "#pass the data into the data transformation function myDataset()\n",
    "trainset = MyDataset(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 161
    },
    "colab_type": "code",
    "id": "_El-OQx9S6RV",
    "outputId": "510b89b9-9a73-46f1-8665-9cfa9edf72a4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.2745, 0.3137, 0.3216,  ..., 0.2039, 0.1686, 0.1608],\n",
       "          [0.2549, 0.2392, 0.2275,  ..., 0.2196, 0.2039, 0.1725],\n",
       "          [0.1961, 0.1686, 0.2118,  ..., 0.1922, 0.2196, 0.1843],\n",
       "          ...,\n",
       "          [0.3569, 0.2549, 0.1647,  ..., 0.2824, 0.2196, 0.1686],\n",
       "          [0.3020, 0.3216, 0.3098,  ..., 0.4118, 0.2745, 0.1804],\n",
       "          [0.3020, 0.2824, 0.3294,  ..., 0.4157, 0.4275, 0.3216]]],\n",
       "        dtype=torch.float64),\n",
       " tensor(0))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#display the first tensor in the available tensors\n",
    "trainset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <b>DataLoader is the pytorch function to in load the training data, in our case it the trainset</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YGeqwGocS6RZ"
   },
   "outputs": [],
   "source": [
    "# Creating a train loader for training :\n",
    "trainloader = DataLoader(trainset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F0hTadccS6Rc"
   },
   "source": [
    "### Creating the Model :\n",
    "\n",
    "#### nn.conv2d\n",
    " >> \tApplies a 2D convolution over an input signal composed of several input planes.\n",
    "<a href=\"https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d\">Reference </a>\n",
    "    \n",
    "### 2.class Model(nn.model):\n",
    "    \n",
    ">> This is the base class for all neural network modules in pytroch.<br>\n",
    "\n",
    "### 3. def forward()\n",
    "\n",
    ">This is a pyrtorch hook. The hook can be a forward hook or a backward hook. The forward hook will be executed when a forward call is executed. The backward hook will be executed in the backward phase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iBati2y6S6Rd"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, 5)\n",
    "        self.max1 = nn.MaxPool2d(3, stride=2)\n",
    "        self.conv2 = nn.Conv2d(64, 64, 5)\n",
    "        self.max2 = nn.MaxPool2d(3,stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 4)\n",
    "        self.fc1 = nn.Linear(128 * 5 * 5 , 3072)\n",
    "        self.fc2 = nn.Linear(3072,7)\n",
    "        self.fc3 = nn.Softmax()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.max1(F.relu(self.conv1(x)))\n",
    "        x = self.max2(F.relu(self.conv2(x)))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.dropout(x)\n",
    "        x = x.view(-1, 128*5*5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the model properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "9zOAhOiDS6Rg",
    "outputId": "58709976-078b-4626-c99a-12ca3b53e317",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (max1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (max2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(4, 4), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=3200, out_features=3072, bias=True)\n",
      "  (fc2): Linear(in_features=3072, out_features=7, bias=True)\n",
      "  (fc3): Softmax(dim=None)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loss function\n",
    "\n",
    " >nn.crossentropyloss This criterion combines nn.LogSoftmax() and nn.NLLLoss() in one single class.\n",
    " This It is useful when training a classification problem with C classes. If provided, the optional argument weight should be a 1D Tensor assigning weight to each of the classes. This will be useful because we have unbalanced training set.\n",
    "\n",
    "## optimizer\n",
    ">The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vx1NY1OyS6Rk"
   },
   "outputs": [],
   "source": [
    "# Definign the loss and optimizer..\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adadelta(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model.cpu()\n",
    ">This is used to move the tensor to cpu()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u0a-wIs-S6Rn"
   },
   "outputs": [],
   "source": [
    "model = model.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training:\n",
    "\n",
    "> If you want to test the accuracy and performance of the model, skip dont run this cell, but if you want to get new model weights, run the cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "E0bKG90cS6Rr",
    "outputId": "e9561523-1670-45b6-85ac-87d36a2a3f03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1  Loss on train set : 1.8751886541193181\n",
      "Epoch : 2  Loss on train set : 1.7722693458597927\n",
      "Epoch : 3  Loss on train set : 1.7433360706676135\n",
      "Epoch : 4  Loss on train set : 1.7301102636544674\n",
      "Epoch : 5  Loss on train set : 1.7229532660010027\n",
      "Epoch : 6  Loss on train set : 1.7214031253481505\n",
      "Epoch : 7  Loss on train set : 1.7221447949740976\n",
      "Epoch : 8  Loss on train set : 1.7082648999763257\n",
      "Epoch : 9  Loss on train set : 1.718548725420566\n",
      "Epoch : 10  Loss on train set : 1.7197442964224041\n",
      "Epoch : 11  Loss on train set : 1.736828591521836\n",
      "Epoch : 12  Loss on train set : 1.7367199032489415\n",
      "Epoch : 13  Loss on train set : 1.736486968722148\n",
      "Epoch : 14  Loss on train set : 1.7492835712942847\n",
      "Epoch : 15  Loss on train set : 1.7670973507478276\n",
      "Epoch : 16  Loss on train set : 1.752630713151738\n",
      "Epoch : 17  Loss on train set : 1.7513837899328766\n",
      "Epoch : 18  Loss on train set : 1.7481499058252563\n",
      "Epoch : 19  Loss on train set : 1.7669415533436386\n",
      "Epoch : 20  Loss on train set : 1.7628909296317958\n",
      "Epoch : 21  Loss on train set : 1.7670205400164327\n",
      "Epoch : 22  Loss on train set : 1.7547438786416556\n",
      "Epoch : 23  Loss on train set : 1.7485457095657866\n",
      "Epoch : 24  Loss on train set : 1.748724354354668\n",
      "Epoch : 25  Loss on train set : 1.7542348171512923\n",
      "Epoch : 26  Loss on train set : 1.7618564870905749\n",
      "Epoch : 27  Loss on train set : 1.7736923027378566\n",
      "Epoch : 28  Loss on train set : 1.7640787760416667\n",
      "Epoch : 29  Loss on train set : 1.748184285699365\n",
      "Epoch : 30  Loss on train set : 1.8075371128565063\n",
      "Epoch : 31  Loss on train set : 1.8072011475044563\n",
      "Epoch : 32  Loss on train set : 1.7617226666945187\n",
      "Epoch : 33  Loss on train set : 1.8298846838096035\n",
      "Epoch : 34  Loss on train set : 1.7853680778952206\n",
      "Epoch : 35  Loss on train set : 1.7640143681762477\n",
      "Epoch : 36  Loss on train set : 1.7777263892950645\n",
      "Epoch : 37  Loss on train set : 1.7852406765262923\n",
      "Epoch : 38  Loss on train set : 1.7566415171359737\n",
      "Epoch : 39  Loss on train set : 1.793472262839795\n",
      "Epoch : 40  Loss on train set : 1.8483767041973038\n",
      "Epoch : 41  Loss on train set : 1.7856417025261808\n",
      "Epoch : 42  Loss on train set : 1.7977522307751226\n",
      "Epoch : 43  Loss on train set : 1.7749922101297906\n",
      "Epoch : 44  Loss on train set : 1.7802095736199308\n",
      "Epoch : 45  Loss on train set : 1.7661332999108734\n",
      "Epoch : 46  Loss on train set : 1.7770576137059937\n",
      "Epoch : 47  Loss on train set : 1.833981328682041\n",
      "Epoch : 48  Loss on train set : 1.7813215884720366\n",
      "Epoch : 49  Loss on train set : 1.8030148538463682\n",
      "Epoch : 50  Loss on train set : 1.783926407921123\n",
      "Epoch : 51  Loss on train set : 1.7765582351548574\n",
      "Epoch : 52  Loss on train set : 1.7725164240056819\n",
      "Epoch : 53  Loss on train set : 1.8217318665747548\n",
      "Epoch : 54  Loss on train set : 1.8254416290663993\n",
      "Epoch : 55  Loss on train set : 1.7658005984709224\n",
      "Epoch : 56  Loss on train set : 1.8406721308907086\n",
      "Epoch : 57  Loss on train set : 1.775412058022783\n",
      "Epoch : 58  Loss on train set : 1.7823708274147727\n",
      "Epoch : 59  Loss on train set : 1.7573514180174912\n",
      "Epoch : 60  Loss on train set : 1.7681338604333778\n",
      "Epoch : 61  Loss on train set : 1.7703414617800246\n",
      "Epoch : 62  Loss on train set : 1.7862865427598595\n",
      "Epoch : 63  Loss on train set : 1.7708553103414661\n",
      "Epoch : 64  Loss on train set : 1.7623861112271613\n",
      "Epoch : 65  Loss on train set : 1.7780681208918225\n",
      "Epoch : 66  Loss on train set : 1.7604831416764148\n",
      "Epoch : 67  Loss on train set : 1.7659888174019607\n",
      "Epoch : 68  Loss on train set : 1.775043344752674\n",
      "Epoch : 69  Loss on train set : 1.7686681628439729\n",
      "Epoch : 70  Loss on train set : 1.764294520631406\n",
      "Epoch : 71  Loss on train set : 1.8549750288965017\n",
      "Epoch : 72  Loss on train set : 1.8892978933405749\n",
      "Epoch : 73  Loss on train set : 1.7657138872061608\n",
      "Epoch : 74  Loss on train set : 1.8026511452414773\n",
      "Epoch : 75  Loss on train set : 1.8112828871783089\n",
      "Epoch : 76  Loss on train set : 1.7970943348930482\n",
      "Epoch : 77  Loss on train set : 1.791937244979668\n",
      "Epoch : 78  Loss on train set : 1.7928316656918448\n",
      "Epoch : 79  Loss on train set : 1.8080875172334558\n",
      "Epoch : 80  Loss on train set : 1.788159375522226\n",
      "Epoch : 81  Loss on train set : 1.8013865968972815\n",
      "Epoch : 82  Loss on train set : 1.7711590717608066\n",
      "Epoch : 83  Loss on train set : 1.7805924305091354\n",
      "Epoch : 84  Loss on train set : 1.776617311963848\n",
      "Epoch : 85  Loss on train set : 1.7748916816371434\n",
      "Epoch : 86  Loss on train set : 1.7921548391195967\n",
      "Epoch : 87  Loss on train set : 1.8531559418866979\n",
      "Epoch : 88  Loss on train set : 1.849093676888369\n",
      "Epoch : 89  Loss on train set : 1.811163536792558\n",
      "Epoch : 90  Loss on train set : 1.8054158963834113\n",
      "Epoch : 91  Loss on train set : 1.7928417838193516\n",
      "Epoch : 92  Loss on train set : 1.808029202003955\n",
      "Epoch : 93  Loss on train set : 1.810324167397783\n",
      "Epoch : 94  Loss on train set : 1.779943673580938\n"
     ]
    }
   ],
   "source": [
    "# Trainign the model\n",
    "nb_epochs = 100\n",
    "model.train()\n",
    "for epoch in range(1, nb_epochs+1):\n",
    "    train_loss = 0\n",
    "    i = 0\n",
    "    for images,label in trainloader:\n",
    "        images = images.cuda().float()\n",
    "        label = label.cuda()\n",
    "        #images = images.float()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(images)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss\n",
    "        i = i + 1\n",
    "    print(\"Epoch :\", epoch, \" Loss on train set :\", train_loss.item()/(i*1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model evaluation\n",
    "\n",
    "> Lets evaluate the accuracy of the model on the traininig dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(model, data, cuda=False):\n",
    "    model.eval()\n",
    "    model.to(device='cpu')    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, (inputs, labels) in enumerate(trained[data]):\n",
    "            if cuda:\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "            # obtain the outputs from the model\n",
    "            outputs = model.forward(inputs)\n",
    "            # max provides the (maximum probability, max value)\n",
    "            _, predicted = outputs.max(dim=1)\n",
    "            # check the \n",
    "            if idx == 0:\n",
    "                print(predicted) #the predicted class\n",
    "                print(torch.exp(_)) # the predicted probability\n",
    "            equals = predicted == labels.data\n",
    "            if idx == 0:\n",
    "                print(equals)\n",
    "            print(equals.float().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src=\"history_training_dataset.png\" width=\"400\" height=\"300\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l0rzd3V9S6Ru"
   },
   "outputs": [],
   "source": [
    "img,lab = next(iter(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8PkcMPYDS6Ry",
    "outputId": "f1c7cb67-05bf-436d-c321-4e1b74856264"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "output = model(img.cpu().float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m2mLXueHS6R1",
    "outputId": "65af37bf-872b-422b-a6c0-6759aaff2459"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = torch.argmax(output, dim=1)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qrW-1qFkS6R5"
   },
   "outputs": [],
   "source": [
    "output = output.cpu().numpy()\n",
    "lab = lab.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wuYsr_uBS6R8",
    "outputId": "62514a0a-96ae-415d-acd4-fb9a118a795c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.25"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Overall accuracy :\n",
    "cor = sum(output == lab)\n",
    "cor/32*100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vjMwuceDS6SA"
   },
   "outputs": [],
   "source": [
    "#torch.save is used to save the model\n",
    "\n",
    "#torch.save(model, 'model_save.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">we add the model.state_dict() parameter to make the model they can be easily saved, updated, altered, and restored, \n",
    "adding a great deal to its modularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ywF8C4NsS6SE"
   },
   "outputs": [],
   "source": [
    " \n",
    "#torch.save(model.state_dict(), 'model_save_state.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jL-CeQFOS6SH",
    "outputId": "d7a4fd4b-9744-4054-f8ca-f1420b0f7817"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (max1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (max2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(4, 4), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=3200, out_features=3072, bias=True)\n",
      "  (fc2): Linear(in_features=3072, out_features=7, bias=True)\n",
      "  (fc3): Softmax(dim=None)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We choose the device, since we will be testing th model on cpu, we use the cpu parameter\n",
    "\n",
    "> Then using the load_state_dict() we will load the already saved model from training of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "vGkxhtNGS6SL",
    "outputId": "bbc36d4d-7acb-45fb-ff25-558ee10ae3ea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "model.load_state_dict(torch.load('model_save_state.pt', map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OYvTYhy3S6SS"
   },
   "source": [
    "\n",
    "# Test case:\n",
    "> FaceCascade is the variable which takes the loaded haarcscade file\n",
    ">> Object Detection using Haar feature-based cascade classifiers is an effective object detection method proposed by Paul Viola and Michael Jones in their paper, “Rapid Object Detection using a Boosted Cascade of Simple Features” in 2001. It is a machine learning based approach where a cascade function is trained from a lot of positive and negative images. It is then used to detect objects in other images.\n",
    "\n",
    "> When this cell is run, its predictions wil be streamed through the webcam, where the precitions based on the 7 classes: angry, disgust, fear, happy, sad, surprise,neutral.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "colab_type": "code",
    "id": "2zEKAOUOS6ST",
    "outputId": "b52d347a-7a37-482d-d045-78c03ace6a4b"
   },
   "outputs": [],
   "source": [
    "faceCascade = cv2.CascadeClassifier(\"haarcascade_frontalface_alt2.xml\")\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "target = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = video_capture.read()\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    faces = faceCascade.detectMultiScale(gray, scaleFactor=1.1)\n",
    "\n",
    "    # Draw a rectangle around the faces\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2, 5)\n",
    "        face_crop = frame[y:y + h, x:x + w]\n",
    "        face_crop = cv2.resize(face_crop, (48, 48))\n",
    "        face_crop = cv2.cvtColor(face_crop, cv2.COLOR_BGR2GRAY)\n",
    "        face_crop = face_crop.astype('float32') / 255\n",
    "        face_crop = face_crop.reshape(1, 1, face_crop.shape[0], face_crop.shape[1])\n",
    "        \n",
    "        face_crop = torch.from_numpy(face_crop)\n",
    "        output = model(face_crop)\n",
    "        output = torch.argmax(output, dim=1).numpy()\n",
    "        \n",
    "        result = target[int(output)]\n",
    "        cv2.putText(frame, result, (x, y), font, 1, (200, 0, 0), 3, cv2.LINE_AA)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything is done, release the capture\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run validation.ipynb to test the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5-Dikt3HS6SW"
   },
   "source": [
    "The generated image is now ploted to be easily accessible through cv2 and matplotlib libries\n",
    "\n",
    "<b>Performance</b>\n",
    "\n",
    "The model performs on genrally 91.3% accuracy on the given video input\n",
    "\n",
    "From the given aims and objectives, the model performed as expected.\n",
    "\n",
    "\n",
    "The code can be used for different functionalitie which need more user information to be detected automatically just from the facial picture or expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Emotion Recognition.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
